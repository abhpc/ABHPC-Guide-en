#! /bin/bash

# This script is for common MPI programs.

# Define job name.
#SBATCH --job-name=test

# Define the partition to submit. The partition name can be obtained by command "sinfo"
#SBATCH --partition=E5-2640V4

# Define the number of nodes allocated.
#SBATCH --nodes=4

# Define the number of cores allocated per node.
#SBATCH --ntasks-per-node=20

# Define the error and output files.
#SBATCH --error=%j.err
#SBATCH --output=%j.out

# Define your MPI commands.
# For example, if you run your program locally as: mpirun -np 2 a.out -in xxx.in
# Then you should define MPI_CMD as "a.out -in xxx.in"
MPI_CMD="a.out -in xxx.in"

# Do not change any lines unless you know what you are doing.

# Generate a MPI nodelist.
CURDIR=`pwd`
rm -rf $CURDIR/nodelist.$SLURM_JOB_ID
NODES=`scontrol show hostnames $SLURM_JOB_NODELIST`
for i in $NODES
do
	echo "$i:$SLURM_NTASKS_PER_NODE" >> $CURDIR/nodelist.$SLURM_JOB_ID
done
# End of generation.

# Run your MPI programs. The output will be written into a file named "[JOBNAME].sta"
mpirun -genv I_MPI_FABRICS=tcp -machinefile $CURDIR/nodelist.$SLURM_JOB_ID $MPI_CMD > $SLURM_JOB_NAME.sta

# Clean the nodelist file after the MPI program done.
rm -rf $CURDIR/nodelist.$SLURM_JOB_ID
